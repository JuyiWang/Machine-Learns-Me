## 第三周笔记 Week_3

### 十、支持向量机(Support Vector Machines)

#### 10.1 优化目标

相比于逻辑回归，SVM采用了新的代价函数:

![SVM-cost](Img/../../../Img/SVM-cost.png)

逻辑回归中的每一项$x$都会为带来损失项，新的损失函数只在$\theta_Tx^{(i)}$大于1或小于-1时才会带来损失项。

$\frac{1}{m}$不影响最小值时$\theta$的取值，因此不需要在损失项内再计算$*\frac{1}{m}$。

同时用心的参数$C$代替了逻辑回归中的超参$\lambda$,这里的C作用类似于$\frac{1}{\lambda}$用来平衡损失项$J$和正则项$R$。但是$C*J+R$与$J+\lambda * R$并不等价。

此时SVM的假设函数和优化目标:

![SVM-h](Img/../../../Img/SVM-Jmin.png)

SVM会直接预测$y$的取值是1还是0，而不是类似逻辑回归输出概率。

#### 10.2 大边界的直观理解

SVM也被称为大间距分类器。

![SVM-noundart](Img/../../../Img/SVM-cost2.png)

当$y$为正样本时，关于$z$的代价函数为$cost_0(z)$,负样本时，代价函数为$cost_1(z)$。

换句话说，一个正样本，希望$\theta^Tx>=1$，反之，如果$y=0$，函数${\cos}t_0{(z)}$的值在$z<=-1$的区间里函数值为0。这是支持向量机的一个有趣性质。如果有一个正样本$y=1$，仅要求$\theta^Tx$大于等于0，就能将该样本恰当分出，这是因为如果$\theta^Tx$\>0，代价函数值为0。一个负样本，仅需要$\theta^Tx$\<=0就可正确分离，但是，支持向量机的要求更高，不仅能正确分开输入的样本，这就相当于在支持向量机中嵌入了一个额外的安全因子（间距因子）。

![SVM-boundary](Img/../../../Img/SVM-boundary.png)

当$C$值很大时，为使整体cost函数的值降到最低，SVM就必须找到使正负样例最大程度区分的$\theta$。对比下图，即为使正负样例都取最大分割距离的黑线。

![SVM-boundary-1](Img/../../../Img/SVM-boundar-1.png)

这个距离叫做支持向量机的间距，这是**支持向量机具有鲁棒性的原因，因为它努力用一个最大间距来分离样本**。因此支持向量机有时被称为大间距分类器。

当$C$取 **非常大** 值时，SVM对异常样本点更为敏感。$C$取不是很大时，可以忽略掉一些异常点，得到更好的的决策边界，甚至不是线性可分数据。

#### 10.3 大边界分类背后的数学（选修）

决策边界和参数$\theta$垂直，$\theta$是决策面的法向量。

![SVM-max-boundary](Img/../../../Img/SVM-max-boundary.png)

最大间距实际为样本点到$\theta$的投影p值之和最大。

#### 10.4 核函数

kernel核函数：不是取代假设函数，而是使用核函数计算新的特征，再代入到假设函数中。

核函数定义新的特征变量，来实现非线性分类。

![svm-kernel](Img/../../../Img/SVM-kernel-1.png)

如上图，跟定三个坐标点（Landmartks），通过计算$x$与$l^{(1)},l^{(2)},l^{(3)}$的相似程度来构造新的特征$f_1,f_2,f_3$.

kernel函数为**高斯核函数（Gausssian Kernel）**

![svm-kernel-2](Img/../../../Img/SVM-kernel-2.jpg)

此时的假设函数：$h_\theta(x) = \theta_0 + \theta_1 f_1 + \theta_2 f_2 + \theta_3 + f_3$，如预测$y = 1$，则 $\theta_0 + \theta_1 f_1 + \theta_2 f_2 + \theta_3 + f_3 > 0$。

**如何选择Landmarks?**

可以直接将部分训练样本作为Landamarks，这样得到的新特征是建立在原有特征与训练集中所有其他特征之间距离的基础上。

**关于核函数的正则化**

对SVM进行正则化时，要根据实际特征$f_{(i)}$数考虑实际的参数量。

针对大样本大参数量问题，SVM还可以使用$\theta^T M \theta$代替$\theta^2$作为正则项。

**参数$C$和高斯相似函数$\sigma$的选择**

$C$较大时，相当于$\lambda$较小，可能会导致过拟合，高方差；

$C$较小时，相当于$\lambda$较大，可能会导致低拟合，高偏差；

$\sigma$较大时，可能会导致低方差，高偏差；

$\sigma$较小时，可能会导致低偏差，高方差。

#### 10.5 使用支持向量机

1. 利用现有软件包求解参数
2. 选择适当的参数$C$
3. 选择适当的kernel
   常用的kernel函数：常用线性核函数、高斯相似核函数
   **如果选择高斯核函数，对于量级差别很大的特征，在使用高斯核函数之前进行特征标准化**

核函数的一些普遍准则：
$n$为特征数，$m$为训练样本数。

(1)如果相较于$m$而言，$n$要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。

(2)如果$n$较小，而且$m$大小中等，例如$n$在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。

(3)如果$n$较小，而$m$较大，例如$n$在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。

**SVM的优化问题是一种凸优化问题，会找到全局最小值。**

### 十一、聚类(Clustering)

#### 11.1 无监督学习：简介

#### 11.2 K-均值算法

#### 11.3 优化目标

#### 11.4 随机初始化

#### 11.5 选择聚类数

### 十二、降维(Dimensionality Reduction)

#### 12.1 动机一：数据压缩

#### 12.2 动机二：数据可视化

#### 12.3 主成分分析问题

#### 12.4 主成分分析算法

#### 12.5 选择主成分的数量

#### 12.6 重建的压缩表示

#### 12.7 主成分分析法的应用建议

### 十三、异常检测(Anomaly Detection)

#### 13.1 问题的动机

#### 13.2 高斯分布

#### 13.3 算法

#### 13.4 开发和评价一个异常检测系统

#### 13.5 异常检测与监督学习对比

#### 13.6 选择特征

#### 13.7 多元高斯分布（选修）

#### 13.8 使用多元高斯分布进行异常检测（选修）