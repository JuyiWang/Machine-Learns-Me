# Life-Long Learning

Continuous Learning, Never Ending Learning, Incremental Learning.

## Knowledge Retention

Don't forget what have studied.

It's not that it can't, it's that it don't

### Multi-task training

![14_1](/Li_DL/Notes/DL_Img/notes14/14_1.png)

**Storage issue and Computation issue.**

### EWC

![14_2](/Li_DL/Notes/DL_Img/notes14/14_2.png)

$b_i$ the importance of the $\theta$ of previous task.

### Generating Data

![14_3](/Li_DL/Notes/DL_Img/notes14/14_6.png)

## Knowledge Transfer

![14_4](/Li_DL/Notes/DL_Img/notes14/14_3.png)

Knowledge cannot transfer across different tasks, and we cannot store all the models.

**Transfer Learning : No guarantee that learning a new task will retain the ability to do the previous tasks.**

![14_5](/Li_DL/Notes/DL_Img/notes14/14_4.png)

### Gradient Episodic Memory

![14_6](/Li_DL/Notes/DL_Img/notes14/14_5.png)

When traning the current task, keep the ability to previous task.

## Model Expansion 

but Parameter Efficiency.

![14_7](/Li_DL/Notes/DL_Img/notes14/14_7.png)

The learning sequence of tasks affect the learning effect.

