{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning HW.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network - Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optimz\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "from gensim.models import word2vec\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "def load_training_data(path):\n",
    "    if 'training_label' in path:\n",
    "        with open(path, 'r', encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split() for line in lines]\n",
    "        X = [line[2:] for line in lines]\n",
    "        Y = [int(line[0]) for line in lines]\n",
    "        return X,Y\n",
    "    else:\n",
    "        with open(path, 'r', encoding = 'utf-8') as f:\n",
    "            lines = f.readlines()\n",
    "            lines = [line.strip('\\n').split() for line in lines]\n",
    "        X = lines\n",
    "        return X\n",
    "\n",
    "def load_testing_data(path):\n",
    "    with open(path, 'r', encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "        seq = [''.join(line.strip('\\n').split(',')[1:]).strip() for line in lines]\n",
    "    X = [line.split() for line in seq]\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class data_processing():\n",
    "    '''\n",
    "    Data processing class,build the embedding vocab and turn the input sequence to the index in vocab.\n",
    "    '''\n",
    "    def __init__(self, embed_dim = 100, seq_len = 32):\n",
    "        self.embed_dim = embed_dim\n",
    "        self.idx = 0\n",
    "        self.seq_len = seq_len\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.embedding_matrix = []\n",
    "\n",
    "    # train the word2vec model\n",
    "    def train_word2vec(self, data, window = 5):\n",
    "        self.word2vec_model = word2vec.Word2Vec(data, size = self.embed_dim, window = window, iter = 10)\n",
    "    \n",
    "    # add <PAD> and <UNK> to embedding matrix\n",
    "    def add_embedding(self, word):\n",
    "        self.word2idx[word] = self.idx\n",
    "        self.idx2word[self.idx] = word\n",
    "        vector = torch.empty(1, self.embed_dim)\n",
    "        torch.nn.init.uniform_(vector)\n",
    "        self.embedding_matrix = torch.cat([self.embedding_matrix, vector], 0)\n",
    "        self.idx += 1\n",
    "\n",
    "    # bulid the embedding matrix for RNN\n",
    "    def build_embedding_matrix(self):\n",
    "        embedding_matrix = []\n",
    "        for _,word in enumerate(self.word2vec_model.wv.vocab):\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            self.idx += 1\n",
    "            embedding_matrix.append(self.word2vec_model[word])\n",
    "        self.embedding_matrix = torch.tensor(embedding_matrix)\n",
    "        self.add_embedding('<PAD>')\n",
    "        self.add_embedding('<UNK>')\n",
    "        return self.embedding_matrix\n",
    "\n",
    "    # pad the input sequence to max length\n",
    "    def pad_sequence(self, sequence):\n",
    "        if len(sequence) >= self.seq_len:\n",
    "            return sequence[:self.seq_len]\n",
    "        else:\n",
    "            for _ in range(self.seq_len - len(sequence)):\n",
    "                sequence.append('<PAD>')\n",
    "            return sequence\n",
    "\n",
    "    # turn the input sequence to the index of vocab\n",
    "    def seq_to_index(self, sequence):\n",
    "        sequence1 = self.pad_sequence(sequence)\n",
    "        seq = [self.word2idx[word] if word in self.word2idx else self.word2idx['<UNK>'] for word in sequence1]\n",
    "        return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset():\n",
    "\n",
    "    def __init__(self, process, X, y = None):\n",
    "        self.X, self.Y = [],[]\n",
    "        for item in X:\n",
    "            self.X.append(np.array(process.seq_to_index(item)))\n",
    "        if y is not None:\n",
    "            self.Y = torch.LongTensor(y)\n",
    "            assert len(self.X) == len(self.Y)\n",
    "        else:\n",
    "            self.Y = y       \n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        if self.Y is not None:\n",
    "            return self.X[index],self.Y[index]\n",
    "        else:\n",
    "            return self.X[index]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_matrix, embed_grade = False, hidden_dim = 128, num_layers = 1, bidirectional = False, dropout = 0.5):\n",
    "        super(RNN, self).__init__()\n",
    "        # 制作 embeddin layer, 并设置是否更改词向量参数\n",
    "        # self.embed = nn.Embedding(embed_matrix.size(0),embed_matrix.size(1))\n",
    "        # self.embed.weight = nn.Parameter(torch.tensor(embed_matrix))\n",
    "        # self.embed.weight.requires_grad = embed_grade\n",
    "        # self.embed_dim = embed_matrix.shape[1]\n",
    "\n",
    "        self.embed = nn.Embedding.from_pretrained(embed_matrix)\n",
    "        self.embed_dim = embed_matrix.shape[1]\n",
    "        \n",
    "        self.LSTM = nn.LSTM(self.embed_dim, hidden_dim, num_layers = num_layers, batch_first = True, bidirectional = bidirectional)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, 2 * hidden_dim),\n",
    "            nn.Linear(2 * hidden_dim, hidden_dim),\n",
    "            nn.Linear(hidden_dim, 2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        tokens = self.embed(torch.tensor(x))\n",
    "        _,(ht,_) = self.LSTM(tokens)\n",
    "        out = self.fc(ht)\n",
    "        return out.reshape(batch_size,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(pre, label):\n",
    "    acc = np.sum(np.argmax(pre.cpu().data.numpy(), axis = 1) == label.numpy())\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train, valid, batch_size, epoch_num, lr, device):\n",
    "    print('Begin to train.')\n",
    "    optimizer = optimz.Adam(model.parameters(), lr = lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_acc = 0\n",
    "    train_len,val_len = len(train) * batch_size, len(valid)\n",
    "    for epoch in range(epoch_num):\n",
    "        # train\n",
    "        model.train()\n",
    "        train_acc, train_loss = 0,0\n",
    "        for idx, (data, label) in enumerate(train):\n",
    "            train_pre = model(data.to(device))\n",
    "            loss = criterion(train_pre, label.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            train_acc += evaluation(train_pre, label)\n",
    "        # validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_acc, val_loss = 0,0\n",
    "            for idx, (data, label) in enumerate(valid):\n",
    "                val_pre = model(data.to(device))\n",
    "                loss = criterion(val_pre, label.to(device))\n",
    "                val_loss += loss.item()\n",
    "                val_acc += evaluation(val_pre, label)\n",
    "\n",
    "        print('Epoch[%02d|%02d] | Train loss is : %.5f | Train acc is : %.3f | Valid loss is : %.5f | Valid acc is : %.3f '% \\\n",
    "            (epoch+1, epoch_num,train_loss / train_len , train_acc / train_len, val_loss / val_len, val_acc / val_len / batch_size))\n",
    "\n",
    "        if val_acc/val_len > best_acc:\n",
    "            best_acc = val_acc/val_len\n",
    "            print('***** The best accuracy in validation set is %.3f'%(val_acc / val_len / batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biu():\n",
    "    file_path = 'ml2020spring-hw4'\n",
    "    label_path = 'training_label.txt'\n",
    "    nolabel_path = 'training_nolabel.txt'\n",
    "    test_path = 'testing_data.txt'\n",
    "\n",
    "    train_label_data, train_label = load_training_data(os.path.join(file_path, label_path))\n",
    "    train_nolabel_data = load_training_data(os.path.join(file_path, nolabel_path))\n",
    "    test_data = load_testing_data(os.path.join(file_path, test_path))\n",
    "\n",
    "    data = train_label_data + train_nolabel_data + test_data\n",
    "    print('Data load successfully.')\n",
    "\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    lr = 1e-6\n",
    "    batch_size = 32\n",
    "    max_seq_len = 32\n",
    "    embed_dim = 100\n",
    "    epoch_num = 10\n",
    "    print(f'Super parameters set successfully, device is {device}.')\n",
    "\n",
    "    data_pro = data_processing(embed_dim = embed_dim, seq_len = max_seq_len)\n",
    "    data_pro.train_word2vec(data)\n",
    "    embedding = data_pro.build_embedding_matrix()\n",
    "    print('Embedding matrix build successfully.')\n",
    "\n",
    "    train_X, val_X, train_y, val_y = train_test_split(train_label_data, train_label, train_size = 0.8)\n",
    "    train_set = TextDataset(data_pro, train_X, train_y)\n",
    "    valid_set = TextDataset(data_pro, val_X, val_y)\n",
    "    train_loader = DataLoader(train_set, batch_size = batch_size, shuffle = True)\n",
    "    valid_loader = DataLoader(valid_set, batch_size = batch_size, shuffle = True)\n",
    "    print('Data Loader build successfully.')\n",
    "\n",
    "    model = RNN(embedding).to(device)\n",
    "    print('RNN model initialize successfully.')\n",
    "    \n",
    "    train(model, train_loader, valid_loader, batch_size, epoch_num, lr, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load successfully.\n",
      "Super parameters set successfully, device is cpu.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Juyi/anaconda3/envs/deeplearing/lib/python3.6/site-packages/ipykernel_launcher.py:33: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding matrix build successfully.\n",
      "Data Loader build successfully.\n",
      "RNN model initialize successfully.\n",
      "Begin to train.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Juyi/anaconda3/envs/deeplearing/lib/python3.6/site-packages/ipykernel_launcher.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[01|10] | Train loss is : 0.02163 | Train acc is : 0.513 | Valid loss is : 0.68928 | Valid acc is : 0.532 \n",
      "***** The best accuracy in validation set is 0.532\n",
      "Epoch[02|10] | Train loss is : 0.01997 | Train acc is : 0.639 | Valid loss is : 0.60088 | Valid acc is : 0.700 \n",
      "***** The best accuracy in validation set is 0.700\n",
      "Epoch[03|10] | Train loss is : 0.01803 | Train acc is : 0.730 | Valid loss is : 0.55895 | Valid acc is : 0.751 \n",
      "***** The best accuracy in validation set is 0.751\n",
      "Epoch[04|10] | Train loss is : 0.01722 | Train acc is : 0.756 | Valid loss is : 0.54051 | Valid acc is : 0.763 \n",
      "***** The best accuracy in validation set is 0.763\n",
      "Epoch[05|10] | Train loss is : 0.01671 | Train acc is : 0.768 | Valid loss is : 0.52972 | Valid acc is : 0.773 \n",
      "***** The best accuracy in validation set is 0.773\n",
      "Epoch[06|10] | Train loss is : 0.01640 | Train acc is : 0.778 | Valid loss is : 0.52292 | Valid acc is : 0.779 \n",
      "***** The best accuracy in validation set is 0.779\n",
      "Epoch[07|10] | Train loss is : 0.01621 | Train acc is : 0.783 | Valid loss is : 0.51933 | Valid acc is : 0.782 \n",
      "***** The best accuracy in validation set is 0.782\n",
      "Epoch[08|10] | Train loss is : 0.01613 | Train acc is : 0.786 | Valid loss is : 0.51756 | Valid acc is : 0.785 \n",
      "***** The best accuracy in validation set is 0.785\n",
      "Epoch[09|10] | Train loss is : 0.01604 | Train acc is : 0.789 | Valid loss is : 0.51557 | Valid acc is : 0.787 \n",
      "***** The best accuracy in validation set is 0.787\n",
      "Epoch[10|10] | Train loss is : 0.01598 | Train acc is : 0.792 | Valid loss is : 0.51304 | Valid acc is : 0.789 \n",
      "***** The best accuracy in validation set is 0.789\n"
     ]
    }
   ],
   "source": [
    "biu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Epoch[01|10] | Train loss is : 0.02145 | Train acc is : 0.549 | Valid loss is : 0.66974 | Valid acc is : 0.601 \n",
    "***** The best accuracy in validation set is 0.601\n",
    "Epoch[02|10] | Train loss is : 0.01922 | Train acc is : 0.676 | Valid loss is : 0.58231 | Valid acc is : 0.716 \n",
    "***** The best accuracy in validation set is 0.716\n",
    "Epoch[03|10] | Train loss is : 0.01802 | Train acc is : 0.728 | Valid loss is : 0.56958 | Valid acc is : 0.739 \n",
    "***** The best accuracy in validation set is 0.739\n",
    "Epoch[04|10] | Train loss is : 0.01778 | Train acc is : 0.740 | Valid loss is : 0.56905 | Valid acc is : 0.740 \n",
    "***** The best accuracy in validation set is 0.740\n",
    "Epoch[05|10] | Train loss is : 0.01750 | Train acc is : 0.749 | Valid loss is : 0.55735 | Valid acc is : 0.751 \n",
    "***** The best accuracy in validation set is 0.751\n",
    "Epoch[06|10] | Train loss is : 0.01740 | Train acc is : 0.751 | Valid loss is : 0.55570 | Valid acc is : 0.753 \n",
    "***** The best accuracy in validation set is 0.753\n",
    "Epoch[07|10] | Train loss is : 0.01726 | Train acc is : 0.756 | Valid loss is : 0.55236 | Valid acc is : 0.756 \n",
    "***** The best accuracy in validation set is 0.756\n",
    "Epoch[08|10] | Train loss is : 0.01714 | Train acc is : 0.760 | Valid loss is : 0.55184 | Valid acc is : 0.757 \n",
    "***** The best accuracy in validation set is 0.757\n",
    "Epoch[09|10] | Train loss is : 0.01704 | Train acc is : 0.762 | Valid loss is : 0.54674 | Valid acc is : 0.760 \n",
    "***** The best accuracy in validation set is 0.760\n",
    "Epoch[10|10] | Train loss is : 0.01692 | Train acc is : 0.764 | Valid loss is : 0.54165 | Valid acc is : 0.762 \n",
    "***** The best accuracy in validation set is 0.762"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
