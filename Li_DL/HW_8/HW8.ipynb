{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HW8.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P70D2YtaPyQx",
        "colab_type": "text"
      },
      "source": [
        "# HW.8 Seq2Seq\n",
        "\n",
        "- 英文翻譯中文\n",
        "\n",
        "    - 输入： 一句英文 （e.g. tom is a student .）\n",
        "    - 输出： 中文翻译 （e.g. 湯姆 是 個 學生 。）\n",
        "- TODO\n",
        "\n",
        "    - 实现 Attention Mechanism\n",
        "    - 实现 Beam Search\n",
        "    - 实现 Schedule Sampling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BY0VZ6qRP5gx",
        "colab_type": "text"
      },
      "source": [
        "## Dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lusOZB2CP4-R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "1828057a-1730-4a96-8e3f-ef56409fc927"
      },
      "source": [
        "!gdown --id '1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg' --output data.tar.gz\n",
        "!tar -zxvf data.tar.gz\n",
        "!mkdir ckpt\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1r4px0i-NcrnXy1-tkBsIwvYwbWnxAhcg\n",
            "To: /content/data.tar.gz\n",
            "\r0.00B [00:00, ?B/s]\r4.72MB [00:00, 38.2MB/s]\r5.83MB [00:00, 35.4MB/s]\n",
            "cmn-eng/\n",
            "cmn-eng/int2word_cn.json\n",
            "cmn-eng/int2word_en.json\n",
            "cmn-eng/preprocess/\n",
            "cmn-eng/preprocess/build_dataset.py\n",
            "cmn-eng/preprocess/build_dictionary.sh\n",
            "cmn-eng/preprocess/cmn.txt\n",
            "cmn-eng/preprocess/cn.txt\n",
            "cmn-eng/preprocess/dict.txt.big\n",
            "cmn-eng/preprocess/dict.txt.small\n",
            "cmn-eng/preprocess/en.txt\n",
            "cmn-eng/preprocess/en_code.txt\n",
            "cmn-eng/preprocess/en_refine.txt\n",
            "cmn-eng/preprocess/en_vocab.txt\n",
            "cmn-eng/preprocess/tokenizer.py\n",
            "cmn-eng/testing.txt\n",
            "cmn-eng/training.txt\n",
            "cmn-eng/validation.txt\n",
            "cmn-eng/word2int_cn.json\n",
            "cmn-eng/word2int_en.json\n",
            "ckpt  cmn-eng  data.tar.gz  drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1IeSzjexQytS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import json\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optimz\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import jieba\n",
        "import random\n",
        "from torch.utils.data import Dataset,DataLoader\n",
        "from gensim.models import word2vec\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "joNuKQFqmMhh",
        "colab_type": "text"
      },
      "source": [
        "## Data preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AVQ_pxyoQyfj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class data_preprocess():\n",
        "    def __init__(self, config):\n",
        "        self.root = config.data_path\n",
        "        self.embed_dim = config.embed_dim\n",
        "        self.cn_int2word, self.cn_word2int = self.get_dictionary('cn')\n",
        "        self.en_int2word, self.en_word2int = self.get_dictionary('en')\n",
        "\n",
        "    def get_dictionary(self, language):\n",
        "        \"\"\"\n",
        "        Load the vocabulary\n",
        "        return:\n",
        "            int2word, word2int : dic of vocabulary\n",
        "        \"\"\"\n",
        "        with open(os.path.join(self.root, f'int2word_{language}.json'), 'r') as f:\n",
        "            int2word = json.load(f)\n",
        "        with open(os.path.join(self.root, f'word2int_{language}.json'), 'r') as f:\n",
        "            word2int = json.load(f)\n",
        "        return int2word, word2int\n",
        "\n",
        "    def train_word2vec(self, en_data, cn_data):\n",
        "        \"\"\"\n",
        "        Train the word2vec model\n",
        "        return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        self.en_word2vec = word2vec.Word2Vec(en_data, size = self.embed_dim, window = 5, min_count = 2)\n",
        "        self.cn_word2vec = word2vec.Word2Vec(cn_data, size = self.embed_dim, window = 5, min_count = 2) \n",
        "\n",
        "    def added_embedding(self):\n",
        "        \"\"\"\n",
        "        Initial tag embedding <PAD><BOS><EOS><UNK>\n",
        "        \"\"\"\n",
        "        pad_vector = torch.empty(1, self.embed_dim)\n",
        "        vector = torch.empty(3, self.embed_dim)\n",
        "        torch.nn.init.uniform_(vector)\n",
        "        return torch.cat([pad_vector, vector], 0)\n",
        "\n",
        "    def build_embedding(self):\n",
        "        \"\"\"\n",
        "        Build the embedding matrix for nn.Embedding\n",
        "        return:\n",
        "            en_embedding,cn_embedding : tensor of vocab_size * embed_dim\n",
        "        \"\"\"\n",
        "        en_add_embedding = self.added_embedding()\n",
        "        en_embedding = np.empty((len(self.en_word2int), self.embed_dim))\n",
        "        for word in self.en_word2int:\n",
        "            en_embedding[self.en_word2int[word],:] = self.en_word2vec[word] if word in self.en_word2vec else en_add_embedding[3,:]\n",
        "        self.en_embed_matrix = torch.cat([en_add_embedding, torch.tensor(en_embedding[4:],dtype = torch.float)], 0) \n",
        "\n",
        "        cn_add_embedding = self.added_embedding()\n",
        "        cn_embedding = np.empty((len(self.cn_word2int), self.embed_dim))\n",
        "        for word in self.cn_word2int:\n",
        "            cn_embedding[self.cn_word2int[word],:] = self.cn_word2vec[word] if word in self.cn_word2vec else cn_add_embedding[3,:]\n",
        "        self.cn_embed_matrix = torch.cat([cn_add_embedding, torch.tensor(cn_embedding[4:],dtype = torch.float)], 0)\n",
        "        return self.en_embed_matrix, self.cn_embed_matrix\n",
        "    \n",
        "    def pad_sequence(self, seq, max_len):\n",
        "        \"\"\"\n",
        "        Padding sequence '[BOS] sequences [EOS]' fixed length = max_len\n",
        "        return:\n",
        "            fixed length sequence\n",
        "        \"\"\"\n",
        "        seq.insert(0,1)\n",
        "        seq_out = [0]*max_len\n",
        "        seq = seq[:max_len-1]\n",
        "        seq.append(2)\n",
        "        seq_out[:len(seq)] = seq\n",
        "        return seq_out\n",
        "\n",
        "    def seq_preprocess(self, sequence, max_len):\n",
        "        \"\"\"\n",
        "        Take the input word sequnence to index sequence and padding to fixed length\n",
        "        return:\n",
        "            en_out,cn_out : list of fixed length\n",
        "        \"\"\"\n",
        "        x = sequence.strip('\\n').split('\\t')\n",
        "        # word to index \n",
        "        en_idx = [self.en_word2int[word] if word in self.en_word2int else self.en_word2int['<UNK>'] for word in x[0].split()]\n",
        "        cn_idx = [self.cn_word2int[word] if word in self.cn_word2int else self.cn_word2int['<UNK>'] for word in x[1].split()]\n",
        "        # padding\n",
        "        en_out = self.pad_sequence(en_idx, max_len)\n",
        "        cn_out = self.pad_sequence(cn_idx, max_len)  \n",
        "        return en_out,cn_out\n",
        "     "
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWdoUGTtmT0B",
        "colab_type": "text"
      },
      "source": [
        "### Load word2vec data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVWqgH6gFIKx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_train_data(path):\n",
        "    \"\"\"\n",
        "    Load the corpis to train the word2vec\n",
        "    return:\n",
        "        en_data, cn_data : corpus for word2vec training\n",
        "    \"\"\"\n",
        "    with open(path, 'r') as f:\n",
        "        en_data,cn_data = [],[]\n",
        "        lines = f.readlines()\n",
        "        for line in lines:\n",
        "            seq = line.strip('\\n').split('\\t')\n",
        "            en_data.append(seq[0].split())\n",
        "            cn_data.append(seq[1].split()) \n",
        "        return en_data,cn_data"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlAoj9NimZ0A",
        "colab_type": "text"
      },
      "source": [
        "### Build data process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HY_AEhBqHEcl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_data_process(path_list, config):\n",
        "    \"\"\"\n",
        "    return:\n",
        "        process : class data_preprocess \n",
        "        en_embedding : english word2vec | array of vocab_size * embed_dim\n",
        "        cn_embedding : chinese word2vec | array of vacab_size * embed_dim\n",
        "    \"\"\"\n",
        "    en_train_vec, cn_train_vec = [],[]\n",
        "    for path in path_list:\n",
        "        en_data, cn_data = load_train_data(path)\n",
        "        en_train_vec += en_data\n",
        "        cn_train_vec += cn_data\n",
        "    \n",
        "    process = data_preprocess(config)\n",
        "    process.train_word2vec(en_train_vec, cn_train_vec)\n",
        "    en_embedding,cn_embedding = process.build_embedding()\n",
        "    return process, en_embedding, cn_embedding"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-WoBUivLZtM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class TextDataSet(Dataset):\n",
        "    def __init__(self, path, process, max_len):\n",
        "        with open(path, 'r') as f:\n",
        "            self.en, self.cn = [],[]\n",
        "            for line in f.readlines():\n",
        "                en_seq, cn_seq = process.seq_preprocess(line, max_len)\n",
        "                self.en.append(en_seq)\n",
        "                self.cn.append(cn_seq)\n",
        "            assert len(self.cn) == len(self.en)\n",
        "    \n",
        "    def __len__(self):\n",
        "        return len(self.en)\n",
        "\n",
        "    def __getitem__(self,index):\n",
        "        return np.array(self.en[index]), np.array(self.cn[index])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkMW9fAOf3Q1",
        "colab_type": "text"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6reZ0eqPYREb",
        "colab_type": "text"
      },
      "source": [
        "### Dynamic RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbyiB9_SYWn-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Dynamic_RNN(nn.Module):\n",
        "    # Dynamic RNN with sequence pad\n",
        "    def __init__(self, input_dim, hidden_dim, Type = 'LSTM', num_layers = 1, nonlinearity = 'tanh', bias = True, batch_first = True, dropout = 0, bidirectional = False):\n",
        "        super(Dynamic_RNN, self).__init__()\n",
        "        self.batch_first = batch_first\n",
        "        self.rnn_type = Type\n",
        "        if Type == 'RNN':\n",
        "            self.RNN = nn.RNN(input_size = input_dim, hidden_size = hidden_dim, num_layers = num_layers,bias = bias, \n",
        "                              dropout = dropout, batch_first = batch_first, bidirectional = bidirectional)\n",
        "        elif Type == 'LSTM':\n",
        "            self.RNN = nn.LSTM(input_size = input_dim, hidden_size = hidden_dim, num_layers = num_layers,bias = bias, \n",
        "                               dropout = dropout, batch_first = batch_first, bidirectional = bidirectional)\n",
        "        else:\n",
        "            self.RNN = nn.GRU(input_size = input_dim, hidden_size = hidden_dim, num_layers = num_layers,bias = bias, \n",
        "                              dropout = dropout, batch_first = batch_first, bidirectional = bidirectional)\n",
        "\n",
        "    def forward(self,x,x_len,hidden):\n",
        "        x_sort_idx = torch.sort(-x_len)[1].long()\n",
        "        x_unsort_idx = torch.sort(x_sort_idx)[1].long()\n",
        "        x_len = x_len[x_sort_idx]\n",
        "        x = x[x_sort_idx]\n",
        "\n",
        "        x_embed_pad = torch.nn.utils.rnn.pack_padded_sequence(x, x_len, batch_first = self.batch_first)\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            out_pack,(h_t,c_t) = self.RNN(x_embed_pad,hidden)\n",
        "        else:\n",
        "            out_pack, h_t = self.RNN(x_embed_pad,hidden)\n",
        "            c_t = None\n",
        "        \n",
        "        h_t = torch.transpose(h_t, 0, 1)[x_unsort_idx] \n",
        "        h_t = torch.transpose(h_t, 0, 1)\n",
        "\n",
        "        out = torch.nn.utils.rnn.pad_packed_sequence(out_pack, batch_first = self.batch_first)\n",
        "        out = out[0]\n",
        "        out = out[x_unsort_idx]\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            c_t = torch.transpose(c_t, 0, 1)[x_unsort_idx]\n",
        "            c_t = torch.transpose(c_t, 0, 1)\n",
        "        return out,(h_t,c_t)  "
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9arEbfEl1Nf",
        "colab_type": "text"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "744Evy87VlZ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    # Encoder Model\n",
        "    def __init__(self, embed_matrix, layers = 1, hidden_dim = 128):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embed_dim = embed_matrix.shape[1]\n",
        "        # self.embed = nn.Embedding.from_pretrained(embed_matrix)\n",
        "        self.embed = nn.Embedding(embed_matrix.shape[0], embed_matrix.shape[1])\n",
        "        self.rnn = Dynamic_RNN(self.embed_dim, hidden_dim, Type = 'GRU',num_layers = layers, bidirectional = True)\n",
        "        # self.rnn = nn.LSTM(self.embed_dim, hidden_dim, num_layers = layers, bidirectional = True)\n",
        "\n",
        "    def forward(self, inputs):\n",
        "        inputs_len = torch.tensor(torch.sum(inputs != 0,dim = -1),dtype = torch.float)\n",
        "        tokens = self.embed(inputs)\n",
        "        out,(h_t,_) = self.rnn(tokens, inputs_len, None)\n",
        "        return out,h_t"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrsjYNN9l7ly",
        "colab_type": "text"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oci-QylmgvcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    # Attention Model\n",
        "    def __init__(self, enc_dim, dec_dim, num_layers):\n",
        "        super(Attention,self).__init__()\n",
        "        self.enc_dim = enc_dim * 2\n",
        "        self.dec_dim = dec_dim * 2\n",
        "        self.softmax = nn.Softmax(dim = 2)\n",
        "        self.dense1 = nn.Linear(self.enc_dim+self.dec_dim,self.dec_dim, bias = False)\n",
        "        self.dense2 = nn.Linear(self.enc_dim * num_layers, self.enc_dim)\n",
        "\n",
        "    def forward(self, k, q, v):\n",
        "        q = self.dense2(q)\n",
        "        q_t = q.permute(1,2,0).contiguous()\n",
        "        score = torch.matmul(k, q_t)\n",
        "        score = score.permute(0,2,1).contiguous()\n",
        "        score = self.softmax(score)\n",
        "        out = torch.matmul(score, v)\n",
        "        q = q.permute(1,0,2).contiguous()   \n",
        "        out = self.dense1(torch.cat([q, out], dim = -1))\n",
        "        return out"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r116kI0rqC6G",
        "colab_type": "text"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGqry7pEa4_8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    # Decoder Model\n",
        "    def __init__(self, embed_matrix, attention, layers = 1, hidden_dim = 128, dropout = 0.5):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embed_dim = embed_matrix.shape[1]\n",
        "        self.out_dim = embed_matrix.shape[0]\n",
        "        self.hid_dim = hidden_dim * 2\n",
        "        # self.embedding = nn.Embedding.from_pretrained(embed_matrix)\n",
        "        self.embedding = nn.Embedding(embed_matrix.shape[0], embed_matrix.shape[1])\n",
        "        self.attn = attention\n",
        "        self.RNN = nn.GRU(self.embed_dim, self.hid_dim, num_layers = layers, dropout = 0.5 ,batch_first = True, bidirectional = False)\n",
        "        self.dense = nn.Sequential(\n",
        "                        nn.Dropout(dropout),\n",
        "                        nn.Linear(self.hid_dim*2, self.hid_dim*4),\n",
        "                        nn.Linear(self.hid_dim*4, self.hid_dim*2),\n",
        "                        nn.Linear(self.hid_dim*2, self.hid_dim),\n",
        "                        nn.Linear(self.hid_dim, self.out_dim),\n",
        "                    )\n",
        "\n",
        "    def forward(self, inputs, hidden, encoder_out):\n",
        "        tokens = self.embedding(inputs)\n",
        "        rnn_out, h_t = self.RNN(tokens, hidden)\n",
        "        h_t = h_t.view(1,tokens.shape[0],-1)\n",
        "        attn_out = self.attn(encoder_out, h_t, encoder_out)\n",
        "        attn_out = attn_out.expand(-1,rnn_out.shape[1],-1)\n",
        "        dense_in = torch.cat([rnn_out, attn_out], dim = -1)\n",
        "        out = self.dense(dense_in) \n",
        "        return out,h_t"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvL3v2fv0ulM",
        "colab_type": "text"
      },
      "source": [
        "### Scheduler sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZyydJZc01wG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def scheudle_sampling(Type,rate = 0.5):\n",
        "    if Type == 'train':\n",
        "        return 0.5\n",
        "    else:\n",
        "        return -1"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuBUr4MU1Mo0",
        "colab_type": "text"
      },
      "source": [
        "### Beam Search"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEHTffoJ1MCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def beam_search(beam_list, k):\n",
        "    pass"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lvil6Ca7l_aO",
        "colab_type": "text"
      },
      "source": [
        "### Seq2Seq"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAah8VJSfP8r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    # Sequence to Sequence Model\n",
        "    def __init__(self, encoder, decoder, device, num_layers):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        self.num_layers = num_layers\n",
        "\n",
        "    def forward(self, inputs, target, teacher_forcing_ratio):\n",
        "        batch_size = target.shape[0]\n",
        "        target_len = target.shape[1]\n",
        "        vocab_size = self.decoder.out_dim\n",
        "\n",
        "        outputs = torch.zeros(batch_size, target_len, vocab_size).to(self.device)\n",
        "        preds = torch.cat([torch.ones(batch_size,1), torch.zeros(batch_size, target_len-1)], dim = 1)\n",
        "\n",
        "        # encoder_out : [batch, seq_len,  num_directions * hidden_size]\n",
        "        # encoder_ht  : [num_layers * num_directions, batch, hidden_size]\n",
        "        encoder_out, encoder_ht = self.encoder(inputs)\n",
        "        encoder_ht = encoder_ht.view(self.num_layers, 2, batch_size, -1).contiguous()\n",
        "        encoder_ht = torch.cat((encoder_ht[:,-2,:,:], encoder_ht[:,-1,:,:]), dim = 2)\n",
        "        \n",
        "        decoder_in = torch.zeros((batch_size,target_len)).long()\n",
        "        decoder_in[:,0] = target[:,0]\n",
        "        for t in range(1,target_len):\n",
        "            decoder_out, hid = self.decoder(decoder_in.to(self.device), encoder_ht, encoder_out)\n",
        "\n",
        "            if t == decoder_out.shape[1]+1:\n",
        "                break\n",
        "            outputs[:,t] = decoder_out[:,t-1]\n",
        "            top = decoder_out.argmax(-1)\n",
        "\n",
        "            if random.random() < teacher_forcing_ratio and torch.sum(target[:,t]) != 0:\n",
        "                decoder_in[:,t] = target[:,t]\n",
        "            else:\n",
        "                decoder_in[:,t] = top[:,t-1]\n",
        "            \n",
        "            preds[:,t] = top[:,t-1]\n",
        "        \n",
        "        return outputs,preds"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARBzB1NOdUan",
        "colab_type": "text"
      },
      "source": [
        "## Utils\n",
        "\n",
        "- Basic operation\n",
        "    - Save Model\n",
        "    - Load Model\n",
        "    - Build Model\n",
        "    - Tokens to Sequence\n",
        "    - Compute BLEU score\n",
        "    \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mk8PiszUeO9o",
        "colab_type": "text"
      },
      "source": [
        "### Save Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXEz5CwIdTyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def save_model(model, store_model_path, step):\n",
        "    torch.save(model.state_dict(), f'{store_model_path}/model_{step}.cpkt')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUsGGK2VeTpv",
        "colab_type": "text"
      },
      "source": [
        "### Load Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZ4Nd_VHeXme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def load_model(model, load_model_path):\n",
        "    model.load_state_dict(torch.load(f'{load_model_path}.ckpt'))\n",
        "    return model"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mj63alUAgLre",
        "colab_type": "text"
      },
      "source": [
        "### Build Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "apiV5jJHgKst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(config,en_embed,cn_embed):\n",
        "    attention = Attention(config.hidden_dim, config.hidden_dim, config.num_layers)\n",
        "    encoder = Encoder(en_embed, layers = config.num_layers, hidden_dim = config.hidden_dim)\n",
        "    decoder = Decoder(cn_embed, attention, layers = config.num_layers, hidden_dim = config.hidden_dim, dropout = config.dropout)\n",
        "    model = Seq2Seq(encoder,decoder,config.device, config.num_layers)\n",
        "    optimizer = optimz.Adam(model.parameters(), lr = config.learning_rate, weight_decay = config.weight_decay)\n",
        "    \n",
        "    if config.load_model:\n",
        "        model = load_model(model, config.load_model_path)\n",
        "\n",
        "    model = model.to(config.device)\n",
        "    return model,optimizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNhl08JWk_pf",
        "colab_type": "text"
      },
      "source": [
        "### Tokens to Sequence"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91C0_m1olAPi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokens_to_sequence(outputs, int2word):\n",
        "    \"\"\"\n",
        "    Transform tokens into a sequence\n",
        "    return :\n",
        "        sentence of word character\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    for tokens in outputs:\n",
        "        sentence = []\n",
        "        for token in tokens:\n",
        "            word = int2word[str(int(token))]\n",
        "            if word == '<EOS>':\n",
        "                break\n",
        "            sentence.append(word)\n",
        "        sentences.append(sentence)\n",
        "    return sentences"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6585w6Cj7Pn",
        "colab_type": "text"
      },
      "source": [
        "### Compute BLEU score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlmj62ebj71Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "from nltk.translate.bleu_score import SmoothingFunction\n",
        "\n",
        "def compute_bleu(sentences, targets):\n",
        "    \"\"\"\n",
        "    Compute the BLEU score between the predict sequence and target\n",
        "    return :\n",
        "        BLEU score of one batch\n",
        "    \"\"\"\n",
        "    score = 0\n",
        "    assert (len(sentences) == len(targets))\n",
        "\n",
        "    def cut_token(sentence):\n",
        "        \"\"\"\n",
        "        Split the sentence into character level list\n",
        "        return:\n",
        "            tmp : the list of character tokens\n",
        "        \"\"\"\n",
        "        tmp = []\n",
        "        for token in sentence:\n",
        "            if token == '<UNK>' or token.isdigit() or len(bytes(token[0], encoding = 'utf-8')) == 1:\n",
        "                tmp.append(token)\n",
        "            else:\n",
        "                tmp += [word for word in token]\n",
        "        return tmp\n",
        "    \n",
        "    for sentence, target in zip(sentences, targets):\n",
        "        sentenc = cut_token(sentence)\n",
        "        target = cut_token(target)\n",
        "        score += sentence_bleu([target], sentence, weights=(1,0,0,0))\n",
        "    return score"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kaPHH5-ciU3H",
        "colab_type": "text"
      },
      "source": [
        "## Config"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-HgvdICiXa_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Config(object):\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.embed_dim = 256\n",
        "        self.hidden_dim = 512\n",
        "        self.num_layers = 3\n",
        "        self.dropout = 0.5\n",
        "        self.learning_rate = 5e-5\n",
        "        self.weight_decay = 0\n",
        "        self.epoch_num = 40\n",
        "        self.max_len = 16\n",
        "        self.load_model = False\n",
        "        self.load_model_path = ''\n",
        "        self.store_model_path = \"./ckpt\"      \n",
        "        self.data_path = \"./cmn-eng\"\n",
        "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "88sYTBJjmflI",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "## Train & Test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PI2zILJ24Bx",
        "colab_type": "text"
      },
      "source": [
        "### Train model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3f75XklP_rlu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train(model, optimizer, train_iter, loss_func, device):\n",
        "    model.train()\n",
        "    # model.zero_grad()\n",
        "    losses = 0.0\n",
        "\n",
        "    for idx,data in enumerate(train_iter):\n",
        "        sources = data[0].to(device)\n",
        "        targets = data[1].to(device)\n",
        "        outputs, preds = model(sources, targets, scheudle_sampling('train'))\n",
        "           \n",
        "        outputs = outputs[:,1:].reshape(-1, outputs.size(2))\n",
        "        targets = targets[:,1:].reshape(-1)\n",
        "        loss = loss_func(outputs, targets)\n",
        "\n",
        "        optimizer.zero_grad() \n",
        "        loss.backward()\n",
        "        grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "        \n",
        "    return model, optimizer, losses / len(train_iter)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8pPmk_-8tue",
        "colab_type": "text"
      },
      "source": [
        "### Test model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XalpH-ae8xpP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(model, test_loader, loss_function, process, device):\n",
        "    model.eval()\n",
        "    loss_sum, bleu_score = 0.0, 0.0\n",
        "    n = 0\n",
        "    for idx,data in enumerate(test_loader):\n",
        "        sources,targets = data[0].to(device),data[1].to(device)\n",
        "        batch_size = sources.shape[0]\n",
        "        outputs, preds = model(sources, targets, scheudle_sampling('val'))\n",
        "\n",
        "        outputs = outputs[:,1:].reshape(-1, outputs.shape[2])\n",
        "        targets = targets[:,1:].reshape(-1)\n",
        "\n",
        "        loss = loss_function(outputs, targets)\n",
        "        loss_sum += loss.item()\n",
        "\n",
        "        targets = targets.view(sources.shape[0], -1)\n",
        "        preds = tokens_to_sequence(preds[:,1:], process.cn_int2word)\n",
        "        sources = tokens_to_sequence(sources, process.en_int2word)\n",
        "        targets = tokens_to_sequence(targets, process.cn_int2word)\n",
        "        \n",
        "        bleu_score += compute_bleu(preds, targets)\n",
        "        n += batch_size\n",
        "    \n",
        "    return loss_sum / len(test_loader), bleu_score / n\n",
        "        "
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLg3Gl6ahBTA",
        "colab_type": "text"
      },
      "source": [
        "### Train process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATgg3wvknpp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train_process(config):\n",
        "    process, en_embed, cn_embed =  build_data_process(['./cmn-eng/training.txt','./cmn-eng/validation.txt'],config)\n",
        "    assert len(process.en_word2int) == en_embed.shape[0]\n",
        "    assert len(process.cn_word2int) == cn_embed.shape[0]\n",
        "\n",
        "    train_data = TextDataSet('./cmn-eng/training.txt', process, config.max_len)\n",
        "    train_loader = DataLoader(train_data, batch_size = config.batch_size, shuffle = True, drop_last = False)\n",
        "\n",
        "    val_data = TextDataSet('./cmn-eng/validation.txt', process, config.max_len)\n",
        "    val_loader = DataLoader(val_data, batch_size = config.batch_size, shuffle = True)\n",
        "\n",
        "    model, optimizer = build_model(config, en_embed, cn_embed)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    print('------------------ Model Info ------------------')\n",
        "    print(model)\n",
        "    print('------------------ Optimizer -------------------')\n",
        "    print(optimizer)\n",
        "    print('------------------ Train Epoch -----------------')\n",
        "    best_loss, best_bleu = 99999,0\n",
        "    for epoch in range(config.epoch_num):\n",
        "        model, optimizer, train_loss = train(model, optimizer, train_loader, criterion, config.device)\n",
        "\n",
        "        val_loss, bleu = test(model, val_loader, criterion, process, config.device)\n",
        "\n",
        "        if val_loss < best_loss and bleu > best_bleu:\n",
        "            best_loss, best_bleu = val_loss, bleu\n",
        "            save_model(model,config.store_model_path,epoch+1)\n",
        "            print(f\"Epoch num is {epoch+1}, Best val loss is {val_loss}, Best Bleu socre is {best_bleu}\")\n",
        "        else:\n",
        "            print(f\"Epoch num is {epoch+1}, Bleu score is {bleu}\")\n",
        "\n",
        "    return model, criterion"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0WgiIt_EI9c",
        "colab_type": "text"
      },
      "source": [
        "### Test process"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HECbX_tEH8s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test_process(config, model, criterion):\n",
        "    process, en_embed, cn_embed = build_data_process(['./cmn-eng/testing.txt'], config)\n",
        "    assert len(process.en_word2int) == en_embed.shape[0]\n",
        "    assert len(process.cn_word2int) == cn_embed.shape[0]\n",
        "\n",
        "    data = TextDataSet('./cmn-eng/testing.txt', process, config.max_len)\n",
        "    data_loader = DataLoader(data, batch_size = config.batch_size, shuffle = True, drop_last = False)\n",
        "\n",
        "    test_loss, bleu_score = test(model, data_loader, criterion, process, config.device)\n",
        "\n",
        "    return test_loss, bleu_score"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbssND_ITdMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main():\n",
        "    config = Config()\n",
        "    model,criterion = train_process(config)\n",
        "    test_loss, bleu_socre = test_process(config, model, criterion)\n",
        "    print('val bleu ',bleu_socre)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YEpP1xj9Tp6j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7761b675-4ee9-4b5e-c497-b5c1c1639462"
      },
      "source": [
        "main()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "------------------ Model Info ------------------\n",
            "Seq2Seq(\n",
            "  (encoder): Encoder(\n",
            "    (embed): Embedding(3922, 256)\n",
            "    (rnn): Dynamic_RNN(\n",
            "      (RNN): GRU(256, 512, num_layers=3, batch_first=True, bidirectional=True)\n",
            "    )\n",
            "  )\n",
            "  (decoder): Decoder(\n",
            "    (embedding): Embedding(3805, 256)\n",
            "    (attn): Attention(\n",
            "      (softmax): Softmax(dim=2)\n",
            "      (dense1): Linear(in_features=2048, out_features=1024, bias=False)\n",
            "      (dense2): Linear(in_features=3072, out_features=1024, bias=True)\n",
            "    )\n",
            "    (RNN): GRU(256, 1024, num_layers=3, batch_first=True, dropout=0.5)\n",
            "    (dense): Sequential(\n",
            "      (0): Dropout(p=0.5, inplace=False)\n",
            "      (1): Linear(in_features=2048, out_features=4096, bias=True)\n",
            "      (2): Linear(in_features=4096, out_features=2048, bias=True)\n",
            "      (3): Linear(in_features=2048, out_features=1024, bias=True)\n",
            "      (4): Linear(in_features=1024, out_features=3805, bias=True)\n",
            "    )\n",
            "  )\n",
            ")\n",
            "------------------ Optimizer -------------------\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    lr: 5e-05\n",
            "    weight_decay: 0\n",
            ")\n",
            "------------------ Train Epoch -----------------\n",
            "Epoch num is 1, Best val loss is 2.688581019639969, Best Bleu socre is 0.20029170871796934\n",
            "Epoch num is 2, Bleu score is 0.2231211092489047\n",
            "Epoch num is 3, Best val loss is 2.4497548937797546, Best Bleu socre is 0.24417434806427613\n",
            "Epoch num is 4, Best val loss is 2.4115444719791412, Best Bleu socre is 0.26008331239201926\n",
            "Epoch num is 5, Best val loss is 2.3817493319511414, Best Bleu socre is 0.2630062522283931\n",
            "Epoch num is 6, Best val loss is 2.324291378259659, Best Bleu socre is 0.2651226556039421\n",
            "Epoch num is 7, Best val loss is 2.272228926420212, Best Bleu socre is 0.2739981894568303\n",
            "Epoch num is 8, Best val loss is 2.1832234114408493, Best Bleu socre is 0.2906261933083618\n",
            "Epoch num is 9, Bleu score is 0.28503825107822467\n",
            "Epoch num is 10, Bleu score is 0.28832780515173206\n",
            "Epoch num is 11, Bleu score is 0.28348270755142346\n",
            "Epoch num is 12, Best val loss is 2.0425251871347427, Best Bleu socre is 0.2959412990151629\n",
            "Epoch num is 13, Bleu score is 0.2956296931285758\n",
            "Epoch num is 14, Bleu score is 0.2896738920146494\n",
            "Epoch num is 15, Bleu score is 0.283947661023402\n",
            "Epoch num is 16, Bleu score is 0.2948039824171533\n",
            "Epoch num is 17, Best val loss is 1.9870655685663223, Best Bleu socre is 0.3034352408150158\n",
            "Epoch num is 18, Bleu score is 0.29017144680056706\n",
            "Epoch num is 19, Bleu score is 0.2933837853955225\n",
            "Epoch num is 20, Bleu score is 0.29540690738781133\n",
            "Epoch num is 21, Best val loss is 1.9426892399787903, Best Bleu socre is 0.3110128986711343\n",
            "Epoch num is 22, Bleu score is 0.3107304503257853\n",
            "Epoch num is 23, Bleu score is 0.30105290983612604\n",
            "Epoch num is 24, Bleu score is 0.3054184105796553\n",
            "Epoch num is 25, Bleu score is 0.2975155374883355\n",
            "Epoch num is 26, Bleu score is 0.3087768386718369\n",
            "Epoch num is 27, Bleu score is 0.31430143809429595\n",
            "Epoch num is 28, Bleu score is 0.3092091523999643\n",
            "Epoch num is 29, Bleu score is 0.31111057694901284\n",
            "Epoch num is 30, Bleu score is 0.3133142201386341\n",
            "Epoch num is 31, Bleu score is 0.30829726462319085\n",
            "Epoch num is 32, Bleu score is 0.31449689877408243\n",
            "Epoch num is 33, Bleu score is 0.3011229425766387\n",
            "Epoch num is 34, Bleu score is 0.3099730101986723\n",
            "Epoch num is 35, Bleu score is 0.31856204728479887\n",
            "Epoch num is 36, Bleu score is 0.32284389359875904\n",
            "Epoch num is 37, Bleu score is 0.3230023356637745\n",
            "Epoch num is 38, Bleu score is 0.31591816671134937\n",
            "Epoch num is 39, Bleu score is 0.3062824511034315\n",
            "Epoch num is 40, Bleu score is 0.3183021787133413\n",
            "val bleu  0.3111666071471068\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}